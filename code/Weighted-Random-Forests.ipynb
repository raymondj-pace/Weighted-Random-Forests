{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc9c7a37-be5f-4f9a-978c-3f6aa346ef00",
   "metadata": {},
   "source": [
    "### Ray Jennings\n",
    "### December 2022\n",
    "### Weighted Random Forests\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90da774-6e50-4cf1-906e-77923677b88d",
   "metadata": {},
   "source": [
    "<p>In this Jupyter notebook a Random Forest method is modified to use weighted weak learners instead of using majority voting. For each weak learner within the random forest, the accuracy of each weak learner is determined during the validation phase. A weight vector is computed based on the weak learner accuracies. There are different ways to compute the weights for the weighted random forest, but I have settled on:</p>\n",
    "    \n",
    "<p style=\"text-align: center;\">\n",
    "$\n",
    "\\begin{align}\n",
    "weight=accuracy^{p}\n",
    "\\end{align}\n",
    "$\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: center;\">or</p>\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "$\n",
    "\\begin{align}\n",
    "weight=\\left\\lvert{\\frac{1}{log(accuracy)}}\\right\\rvert\n",
    "\\end{align}\n",
    "$\n",
    "</p>\n",
    "\n",
    "&nbsp;\n",
    "<p>(or other method can be used).</p>\n",
    "    \n",
    "<p>In the first weight equation if <i>p</i> is equal to 1, then that represents the traditional majority voting random forest. When <i>p</i> equals 2, I feel that is the best overall choice. As the value of <i>p</i> increases, the values within the weight vector decrease. But in addition, the separation between the weights increases which gives more weight to the higher accuracy weak learner trees. The fact that the weight values decrease as the value of <i>p</i> increases is not an issue since the most accurate weak learner will have the highest weight and therefore the greatest impact to the final prediction. The simple fact that the weight value is lower when <i>p</i> = 3 then when <i>p</i> equals 2 does not degrade the function of the weights since all of the weights are lower.\n",
    "    \n",
    "The weight vector can be calculated in other ways besides the two examples listed above and the weights are not limited to weight values less than or equal to 1. However, if there is no upper bound on the weights then the prediction values could be negatively altered particularly if one were to let the weight values exceed the maximum integer value. If weight values greater than 1 were used then I would prefer to scale them with something like a min-max scaler.\n",
    "    \n",
    "The weighted random forest classifier implementation used here was modified from and based on the radom forest classifier by: Jason Brownlee, PhD at https://machinelearningmastery.com/\n",
    "    \n",
    "The code below compares three different Random Forest implementations:</p>\n",
    "\n",
    "1. Basic Random Forest\n",
    "2. Weighted Random Forest\n",
    "3. The Scikit-learn RandomForestClassifer (considered to be the \"heavyweight\", ie. the best)\n",
    "\n",
    "\n",
    "The datasets used were all obtained from the University of California, Irivine's Center for Machine Learning and Intelligent Systems.\n",
    "\n",
    "1. Sonar - Rock or Mine Dataset\n",
    "2. Palmer Penguins Species\n",
    "3. Iris Flowers\n",
    "4. White Wine Attributes\n",
    "5. Red Wine Attributes\n",
    "\n",
    "Each of the three tree implemenatations is run on the same permutation of the dataset for 5 k-fold cross-validations with an iteration on the number of trees from 5, 10, 15, 20, 25.\n",
    "\n",
    "The results from these tests look promising. Frequently, the weighted random forest could beat the accuracy of both of the other trees. There were some ties between either the clasic random forest and the weighted random forest, or between the weighted random forest and scikit-learn RandomForestClassifer. But rarely does the weighted random forest come in last place. The \"_best_\" indicator is calculated by taking the average of the 5 accuracy scores for each of the k-fold cross-validations. Alternatively, a simple count could be used to determine the winner for that round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a8cbb5-d762-416b-b5e4-71be35d71556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  \n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "import operator\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910bb81-15f5-4363-89dd-03a7da99a5b1",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### Methods to modify fields of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c0c0c7-e1fe-426d-8a40-b0ad349d3022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load a CSV file\n",
    "#\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "#\n",
    "# Convert string column to float\n",
    "#\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "\n",
    "#\n",
    "# Convert string column to integer\n",
    "#\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30ac67-1c1c-4f64-81c5-f5d7e22d5a6c",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### Methods to load each dataset: Rock-Mine, Penguins, Iris, White Wine, Red Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb19cb5-5f4d-43f5-b148-39b8474790ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Sonar - Rock or Mine Dataset\n",
    "#\n",
    "def load_sonar_dataset():\n",
    "\n",
    "    filename = '../data/sonar.all-data.csv'\n",
    "\n",
    "    dataset = load_csv(filename)\n",
    "\n",
    "    # convert string attributes to integers\n",
    "    for i in range(0, len(dataset[0])-1):\n",
    "        str_column_to_float(dataset, i)\n",
    "\n",
    "    # convert class column to integers\n",
    "    str_column_to_int(dataset, len(dataset[0])-1)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "#\n",
    "# Penguins Species Dataset\n",
    "#\n",
    "def load_penguins_dataset():\n",
    "    \n",
    "    filename = '../data/penguins_lter.csv'\n",
    "\n",
    "    df = pd.read_csv('penguins_lter.csv', sep=',')\n",
    "    df.drop(['Individual ID', 'Clutch Completion', 'studyName', 'Sample Number', 'Stage', 'Date Egg', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)', 'Comments'], inplace=True, axis=1)\n",
    "    cols = list(df.columns.values)\n",
    "    cols.pop(cols.index('Species'))\n",
    "    cols.pop(cols.index('Region'))\n",
    "    cols.pop(cols.index('Island'))\n",
    "    df = df[cols+['Region', 'Island', 'Species']]  # Reorder with target column at end\n",
    "    df.dropna(axis=0, how='any', inplace=True)\n",
    "    df.to_csv('penguins.csv', sep=',', index=False, header=False)\n",
    "\n",
    "    dataset = load_csv('penguins.csv')\n",
    "\n",
    "    # convert string attributes to floats\n",
    "    for i in range(0, len(dataset[0])-4):\n",
    "        str_column_to_float(dataset, i)\n",
    "\n",
    "    # convert class column to integers\n",
    "    str_column_to_int(dataset, len(dataset[0])-4)\n",
    "    str_column_to_int(dataset, len(dataset[0])-3)\n",
    "    str_column_to_int(dataset, len(dataset[0])-2)\n",
    "    str_column_to_int(dataset, len(dataset[0])-1)\n",
    "    \n",
    "    #print(\"DATASET\")\n",
    "    #print(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "#\n",
    "# Iris dataset\n",
    "#\n",
    "def load_iris_dataset():\n",
    "   \n",
    "    filename = '../data/bezdekIris.csv'\n",
    "    \n",
    "    dataset = load_csv(filename)\n",
    "\n",
    "    # convert string attributes to floats\n",
    "    for i in range(0, len(dataset[0])-1):\n",
    "        str_column_to_float(dataset, i)\n",
    "\n",
    "    # convert class column to integers\n",
    "    str_column_to_int(dataset, len(dataset[0])-1)\n",
    "    \n",
    "    return dataset    \n",
    "\n",
    "\n",
    "#\n",
    "# White Wine dataset\n",
    "#\n",
    "def load_white_wine_dataset():\n",
    "    \n",
    "    dataset = load_csv('../data/winequality-white-adj.csv')\n",
    "\n",
    "    # convert string attributes to floats\n",
    "    for i in range(0, len(dataset[0])-1):\n",
    "        str_column_to_float(dataset, i)\n",
    "\n",
    "    # convert class column to integers\n",
    "    str_column_to_int(dataset, len(dataset[0])-1)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "#\n",
    "# Red Wine dataset\n",
    "#\n",
    "def load_red_wine_dataset():\n",
    "    \n",
    "    dataset = load_csv('../data/winequality-red-adj.csv')\n",
    "\n",
    "    # convert string attributes to floats\n",
    "    for i in range(0, len(dataset[0])-1):\n",
    "        str_column_to_float(dataset, i)\n",
    "\n",
    "    # convert class column to integers\n",
    "    str_column_to_int(dataset, len(dataset[0])-1)\n",
    "    \n",
    "    return dataset\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed9dc2b-b7cd-40f0-b95e-7be540c8600f",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### Random Forest Classifer source code by Jason Brownlee, PhD\n",
    "### https://machinelearningmastery.com/implement-random-forest-scratch-python/ ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f6e829-e220-4e10-9187-c7d107317044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Split a dataset into k folds\n",
    "#\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "\n",
    "#\n",
    "# Calculate accuracy percentage\n",
    "#\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    val = correct / float(len(actual))\n",
    "    return val\n",
    "\n",
    "\n",
    "#\n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "#\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "\n",
    "#\n",
    "# Calculate the Gini index for a split dataset\n",
    "#\n",
    "def gini_index(groups, classes):\n",
    "    # count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        # avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        # score the group based on the score for each class\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p * p\n",
    "        # weight the group score by its relative size\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "        \n",
    "    return gini\n",
    "\n",
    "\n",
    "#\n",
    "# Select the best split point for a dataset\n",
    "#\n",
    "def get_split(dataset, n_features):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    features = list()\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0])-1)\n",
    "        if index not in features:\n",
    "            features.append(index)\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "\n",
    "#\n",
    "# Create a terminal node value\n",
    "#\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "\n",
    "#\n",
    "# Create child splits for a node or make terminal\n",
    "#\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, n_features)\n",
    "        split(node['left'], max_depth, min_size, n_features, depth+1)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth+1)\n",
    "\n",
    "\n",
    "#\n",
    "# Build a decision tree\n",
    "#\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    root = get_split(train, n_features)\n",
    "    split(root, max_depth, min_size, n_features, 1)\n",
    "    return root\n",
    "\n",
    "\n",
    "#\n",
    "# Make a prediction with a decision tree\n",
    "#\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "\n",
    "#\n",
    "# Create a random subsample from the dataset with replacement\n",
    "#\n",
    "def subsample(dataset, ratio):\n",
    "    sample = list()\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while len(sample) < n_sample:\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample\n",
    "\n",
    "\n",
    "#\n",
    "# Make a prediction with a list of bagged trees\n",
    "#\n",
    "def bagging_predict(trees, row):\n",
    "    predictions = [predict(tree, row) for tree in trees]\n",
    "    #print(predictions)\n",
    "    rval = max(set(predictions), key=predictions.count)\n",
    "    #print(rval)\n",
    "    return rval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b10054-8a2e-44d2-afc3-552ca29aa1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Adjust weights - raise to power p\n",
    "#\n",
    "def adjust_weights(weights, p): \n",
    "    #for i in range(2, 11):\n",
    "    w = [x**p for x in weights]\n",
    "    return w\n",
    "\n",
    "#\n",
    "# L2 Normalized weights\n",
    "#\n",
    "def adjust_weights_l2norm(weights):\n",
    "    w = weights / np.linalg.norm(weights)\n",
    "    return w\n",
    "\n",
    "\n",
    "#\n",
    "# Weighted Random Forest\n",
    "#\n",
    "def weighted_random_forest(p_val, trees, train, test, actual, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    \n",
    "    predict_arr = [[-1 for i in range(len(trees))] for j in range(len(test))]\n",
    "    \n",
    "    tree_count = 0\n",
    "    num_correct = list()\n",
    "    for t in trees:\n",
    "        n_correct = 0\n",
    "        row_count = 0\n",
    "        for row in test:\n",
    "            p = predict(t, row)\n",
    "            if p == actual[row_count]:\n",
    "                n_correct = n_correct + 1\n",
    "            predict_arr[row_count][tree_count] = p\n",
    "            row_count = row_count + 1\n",
    "        \n",
    "        #print(\"Num correct: \" + str(n_correct) + \" for tree: \" + str(tree_count))\n",
    "        num_correct.append(n_correct)\n",
    "        \n",
    "        #print(predict_arr)\n",
    "        tree_count = tree_count + 1\n",
    "     \n",
    "    #print(\"Best correct: \" + str(num_correct))\n",
    "    \n",
    "    max_correct = max(num_correct)\n",
    "    # print(max_correct)\n",
    "    best_tree_idx = num_correct.index(max(num_correct))\n",
    "    ### print(\"Best tree: \" + str(best_tree_idx) + \" with \" + str(max_correct))\n",
    "    \n",
    "    #weights = [(nc / max_correct / len(trees)) for nc in num_correct]\n",
    "    weights = [(nc / len(test) ) for nc in num_correct]\n",
    "    print()\n",
    "    print(\"Original Tree Weights:\")\n",
    "    print(weights)\n",
    "    print()\n",
    "    weights = adjust_weights(weights, p_val)\n",
    "    print(\"Adjusted Tree Weights:\")\n",
    "    print(weights)\n",
    "    print()\n",
    "    \n",
    "    new_predictions = []\n",
    "    \n",
    "    for q in predict_arr:\n",
    "        dict = {}\n",
    "        for i in range(len(trees)):\n",
    "            \n",
    "            if q[i] in dict.keys():\n",
    "                dict[q[i]] = dict[q[i]] + weights[i]\n",
    "            else:\n",
    "                dict[q[i]] = weights[i]\n",
    "                \n",
    "        max_key = max(dict.items(), key=operator.itemgetter(1))[0]\n",
    "        new_predictions.append(max_key)\n",
    "        # print(\"New prediction: \" + str(max_key))\n",
    "    \n",
    "    return new_predictions\n",
    "    \n",
    "\n",
    "#\n",
    "# Original Random Forest\n",
    "#\n",
    "def random_forest(p_val, trees, train, test, actual, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e93394-2cdb-4bc0-b639-e75eb6356cf8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Perform evaluation on: _Random Forest_, _Weighted Random Forest_, _scikit-learn's RandomForestClassifer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1367969-051a-417a-99c1-8a125a68b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluate both random forests using a cross validation split\n",
    "#\n",
    "def evaluate_RF_algorithms(p_val, dataset, algorithm1, algorithm2, n_folds, *args):\n",
    "    \n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    \n",
    "    scores_rf = list()   # Scores from generic RF\n",
    "    scores_wrf = list()  # Scores from weighted RF\n",
    "    scores_skl = list()  # Scores from scikit-learn's RFC\n",
    "    \n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        \n",
    "        #\n",
    "        # Save a separate copy of the training set for scikit-learn's RandomForestClassifier  \n",
    "        #   - same train_X and train_y but for for scikit-learn only\n",
    "        #\n",
    "        train_X_skl = copy.deepcopy(train_set)\n",
    "        train_y_skl = [row[-1] for row in train_X_skl]\n",
    "        \n",
    "        train_X_mod = list()\n",
    "        train_y_mod = list()\n",
    "        for row in train_X_skl:\n",
    "            row_copy = list(row)\n",
    "            train_y_mod.append(row_copy[-1])\n",
    "            del row_copy[-1]\n",
    "            train_X_mod.append(row_copy)\n",
    "        \n",
    "        test_y = [row[-1] for row in fold]\n",
    "        \n",
    "        test_X = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            del row_copy[-1]\n",
    "            test_X.append(row_copy)\n",
    "            #row_copy[-1] = None\n",
    "        \n",
    "        # Create trees\n",
    "        trees = list()\n",
    "        for i in range(n_trees):\n",
    "            sample = subsample(train_set, sample_size)\n",
    "            tree = build_tree(sample, max_depth, min_size, n_features)\n",
    "            trees.append(tree)\n",
    "        \n",
    "        \n",
    "        predicted_rf = algorithm1(p_val, trees, train_set, test_X, test_y, *args)\n",
    "        accuracy_rf = accuracy_metric(test_y, predicted_rf)\n",
    "        scores_rf.append(accuracy_rf)\n",
    "        \n",
    "        predicted_wrf = algorithm2(p_val, trees, train_set, test_X, test_y, *args)\n",
    "        accuracy_wrf = accuracy_metric(test_y, predicted_wrf)\n",
    "        scores_wrf.append(accuracy_wrf)\n",
    "        \n",
    "        #\n",
    "        # Test scikiet-learn RFC\n",
    "        #\n",
    "        rfc = RandomForestClassifier(n_estimators=n_trees, criterion='gini', max_depth=max_depth, max_features='sqrt', bootstrap=False, oob_score=False, warm_start=False, \n",
    "                                     max_samples=None, min_samples_leaf=min_size, n_jobs=1, random_state=0, verbose=0)\n",
    "        rfc.fit(train_X_mod, train_y_mod)\n",
    "        predict = rfc.predict(test_X)\n",
    "        accuracy_skl_rf = np.mean(predict == test_y)\n",
    "        scores_skl.append(accuracy_skl_rf)\n",
    "        \n",
    "        \n",
    "    return scores_rf, scores_wrf, scores_skl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720718f1-3b4c-4608-abb7-a66bb492e4bc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Perform tests...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a65c0e-2451-44b4-9c45-887adcb48d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "***********\n",
      "For p = 1...\n",
      "***********\n",
      "For 5 trees...\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Test and evaluate the 3 different random forests from the following datasets\n",
    "#\n",
    "\n",
    "seed(2)\n",
    "\n",
    "#\n",
    "# Pick one dataset at a time to evalutate\n",
    "#\n",
    "\n",
    "dataset = load_sonar_dataset()\n",
    "# dataset = load_penguins_dataset()\n",
    "# dataset = load_iris_dataset()\n",
    "# dataset = load_red_wine_dataset()\n",
    "# dataset = load_white_wine_dataset()\n",
    "\n",
    "\n",
    "#\n",
    "# hyper-params\n",
    "#\n",
    "n_folds = 5\n",
    "max_depth = 10\n",
    "min_size = 1  # For SKL: min_samples_leaf, default=1\n",
    "sample_size = 1.0  # Ratio of data to build the tree, for SKL: max_samples, default=None == use X.shape[0] samples\n",
    "n_features = int(sqrt(len(dataset[0])-1))   # For SKL: max_features, default=”sqrt”\n",
    "folds = cross_validation_split(dataset, n_folds)\n",
    "\n",
    "\n",
    "#\n",
    "# Count how many trees in each Random Forest are better than the other Random Forest\n",
    "#\n",
    "\n",
    "# Alter this list based on the dataset\n",
    "for p_val in [1, 2, 3, 4, 5, 10]:\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    print(\"***********\")\n",
    "    print(\"For p = \" + str(p_val) + \"...\")\n",
    "    print(\"***********\")\n",
    "\n",
    "    for n_trees in [5, 10, 15, 20, 25]:\n",
    "\n",
    "        print(\"For \" + str(n_trees) + \" trees...\")\n",
    "    \n",
    "        #\n",
    "        # Evaluate all 3 Random Forest implementations\n",
    "        #\n",
    "        scores_rf, scores_wrf, scores_skl = evaluate_RF_algorithms(p_val, dataset, random_forest, weighted_random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
    "    \n",
    "    \n",
    "        print(\"\\n\\nRESULTS:\")\n",
    "    \n",
    "        rf_avg = 0.0\n",
    "        wrf_avg = 0.0\n",
    "        skl_avg = 0.0\n",
    "    \n",
    "        rf_count = 0\n",
    "        wrf_count = 0\n",
    "        for i in range(n_folds):\n",
    "        \n",
    "            rf_avg += scores_rf[i]\n",
    "            wrf_avg += scores_wrf[i]\n",
    "            skl_avg += scores_skl[i]\n",
    "        \n",
    "            print(str(round(scores_rf[i], 5)) + \" \" + str(round(scores_wrf[i], 5)))\n",
    "            if str(scores_wrf[i]) == str(scores_rf[i]):\n",
    "                print(\"EQUAL\")\n",
    "            elif scores_rf[i] > scores_wrf[i]:\n",
    "                rf_count += 1\n",
    "                print(\"ORIGINAL RF is better\")\n",
    "            else:\n",
    "                wrf_count += 1\n",
    "                print(\"WEIGHTED RF is better\")\n",
    "            \n",
    "            s = \"\"\n",
    "            if rf_count == wrf_count:\n",
    "                s = \"EQUAL\"\n",
    "            elif rf_count > wrf_count:\n",
    "                s = \"ORIGINAL RF is better\"\n",
    "            else:\n",
    "                s = \"WEIGHTED RF is better\"\n",
    "    \n",
    "        print(\"\\n\\n\")\n",
    "        print(\"********************************************\")\n",
    "        print(\"** For p=\" + str(p_val) + \" for \" + str(n_trees) + \" trees \" + s + \" **\")\n",
    "        print(\"********************************************\\n\")\n",
    "    \n",
    "        rf_avg = rf_avg / 5\n",
    "        wrf_avg = wrf_avg / 5\n",
    "        skl_avg = skl_avg / 5\n",
    "    \n",
    "        print()\n",
    "              \n",
    "        print(\"\\nRF Scores:\")\n",
    "        for i in range(0, 5):\n",
    "              print(str(round(scores_rf[i], 5)))\n",
    "        print()\n",
    "        print(\" RF Avg: \" + str(round(rf_avg, 5)))\n",
    "              \n",
    "        print(\"\\nWRF Scores:\")\n",
    "        for i in range(0, 5):\n",
    "              print(str(round(scores_wrf[i], 5)))\n",
    "        print()\n",
    "        print(\"WRF Avg: \" + str(round(wrf_avg, 5)))\n",
    "    \n",
    "        print(\"\\nSci-kit Learn Scores:\")\n",
    "        for i in range(0, 5):\n",
    "              print(str(round(scores_skl[i], 5)))\n",
    "        print()\n",
    "        print(\"SKL Avg: \" + str(round(skl_avg, 5)))\n",
    "        \n",
    "        print(\"\\n**************************************************************************\\n\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d7755-6d7a-4846-8380-67b14671a8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
